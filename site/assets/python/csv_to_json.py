import csv
import json
import re
from datetime import datetime, timedelta
from collections import defaultdict

CSV_FILE = "history.csv"

# –ü—Ä–∞–≤–∏–ª–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö –ø–æ entity_id
DATA_PATTERNS = {
    "batteryLevel": {
        "keywords": ["battery_level", "battery"],
        "file": "ha-battery.json"
    },
    "temperature": {
        "keywords": ["temperature", "_temp"],
        "file": "ha-temperature.json"
    },
    "chargeCurrent": {
        "keywords": ["current", "charge_current"],
        "file": "ha-charge.json"
    },
    "humidity": {
        "keywords": ["humidity", "relative_humidity"],
        "file": "ha-humidity.json"
    }
}

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –¥–≤–æ–π–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤ - —Ç–µ–ø–µ—Ä—å —Å —É–º–Ω—ã–º –ø–æ–∏—Å–∫–æ–º
DUAL_CHARTS = {
    "humidity": {
        "type": "humidity",
        "unit": "%",
        "output": "humidity-in-out.json",
        "max_gap_seconds": 7200,  # 2 —á–∞—Å–∞
    },
    "temperature": {
        "type": "temperature",
        "unit": "¬∞C",
        "output": "temperature-in-out.json",
        "max_gap_seconds": 7200,  # 2 —á–∞—Å–∞
    }
}

# –ú–∞—Ä–∫–µ—Ä—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö/–≤–Ω–µ—à–Ω–∏—Ö —Å–µ–Ω—Å–æ—Ä–æ–≤
LOCATION_MARKERS = {
    "inside": ["inside", "indoor", "interior", "internal", "innen", "drinnen", 
               "room", "raum", "zimmer", "wohnzimmer", "schlafzimmer",
               "case", "gehause", "box", "enclosure",
               "meshtastic", "gateway", "gway", "node", "environment"],
    "outside": ["outside", "outdoor", "exterior", "external", "aussen", "au√üen", 
                "draussen", "drau√üen", "ausentemperatur", "au√üentemperatur",
                "garden", "garten", "balkon", "balcony", "terrace", "terrasse"]
}

def detect_data_type(entity_id: str) -> str | None:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –ø–æ entity_id"""
    entity_lower = entity_id.lower()
    
    for data_type, config in DATA_PATTERNS.items():
        for keyword in config["keywords"]:
            if keyword in entity_lower:
                return data_type
    return None

def detect_location(entity_id: str) -> str | None:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ª–æ–∫–∞—Ü–∏—é —Å–µ–Ω—Å–æ—Ä–∞ (inside/outside) –ø–æ –º–∞—Ä–∫–µ—Ä–∞–º –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç 'inside', 'outside' –∏–ª–∏ None.
    """
    entity_lower = entity_id.lower()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∞—Ä–∫–µ—Ä—ã
    inside_score = sum(1 for marker in LOCATION_MARKERS["inside"] if marker in entity_lower)
    outside_score = sum(1 for marker in LOCATION_MARKERS["outside"] if marker in entity_lower)
    
    if inside_score > outside_score:
        return "inside"
    elif outside_score > inside_score:
        return "outside"
    
    return None

def parse_time(s: str) -> datetime:
    s = (s or "").strip()
    s = s.replace("Z", "")
    return datetime.fromisoformat(s)

def clean_ha_number(s: str) -> float:
    """
    –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫–∏ –≤–∏–¥–∞:
      -12.299.999.949.722.200  -> -12.2999999497222
       20.251.068.571.847.200  ->  20.2510685718472
    –∏ –æ–±—ã—á–Ω—ã–µ —á–∏—Å–ª–∞ —Ç–æ–∂–µ –ø–æ–Ω–∏–º–∞–µ—Ç.
    –ü—Ä–æ–ø—É—Å–∫–∞–µ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: unavailable, unknown, none, null, etc.
    """
    s = (s or "").strip()
    if not s:
        raise ValueError("empty number")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    s_lower = s.lower()
    invalid_values = ["unavailable", "unknown", "none", "null", "nan", "n/a", "na"]
    if s_lower in invalid_values:
        raise ValueError(f"invalid value: {s}")

    # –µ—Å–ª–∏ —É–∂–µ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π float ‚Äî –ø—Ä–æ—Å—Ç–æ –ø–∞—Ä—Å–∏–º
    try:
        return float(s)
    except Exception:
        pass

    # –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã, –∑–Ω–∞–∫ –∏ —Ç–æ—á–∫–∏
    s = re.sub(r"[^0-9\.\-+]", "", s)

    # –µ—Å–ª–∏ —Ç–æ—á–µ–∫ 0 –∏–ª–∏ 1 ‚Äî —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Å–ª—É—á–∞–π
    if s.count(".") <= 1:
        return float(s)

    # –µ—Å–ª–∏ —Ç–æ—á–µ–∫ –º–Ω–æ–≥–æ: —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ –ü–ï–†–í–ê–Ø —Ç–æ—á–∫–∞ ‚Äî –¥–µ—Å—è—Ç–∏—á–Ω–∞—è,
    # –æ—Å—Ç–∞–ª—å–Ω—ã–µ ‚Äî –º—É—Å–æ—Ä–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ ‚Üí —É–±–∏—Ä–∞–µ–º
    sign = ""
    if s[0] in "+-":
        sign, s = s[0], s[1:]

    first_dot = s.find(".")
    if first_dot == -1:
        return float(sign + s)

    int_part = s[:first_dot]
    frac_part = s[first_dot+1:].replace(".", "")
    # –∑–∞—â–∏—Ç–∞: –µ—Å–ª–∏ –≤–¥—Ä—É–≥ int_part –ø—É—Å—Ç–æ–π
    if int_part == "":
        int_part = "0"

    return float(f"{sign}{int_part}.{frac_part}")

def sniff_dialect(path: str):
    with open(path, "r", encoding="utf-8", newline="") as f:
        sample = f.read(4096)
    return csv.Sniffer().sniff(sample, delimiters=",;\t")

def build_aligned_series(series_data: dict, grid_times: list, max_gap_seconds: int) -> list:
    """
    –°—Ç—Ä–æ–∏—Ç –º–∞—Å—Å–∏–≤ –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è —Å–µ—Ä–∏–∏, –≤—ã—Ä–æ–≤–Ω–µ–Ω–Ω—ã–π –ø–æ –æ–±—â–µ–π —Å–µ—Ç–∫–µ –≤—Ä–µ–º–µ–Ω–∏.
    
    Args:
        series_data: dict —Å 'times' –∏ 'values'
        grid_times: –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–æ—á–µ–∫
        max_gap_seconds: –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –¥–æ–ø—É—Å—Ç–∏–º—ã–π —Ä–∞–∑—Ä—ã–≤ –¥–ª—è –ø—Ä–æ—Ç—è–∂–∫–∏ –∑–Ω–∞—á–µ–Ω–∏—è
    
    Returns:
        list –∑–Ω–∞—á–µ–Ω–∏–π (—Å None –¥–ª—è –ø—Ä–æ–ø—É—Å–∫–æ–≤)
    """
    times = series_data['times']
    values = series_data['values']
    
    if not times:
        return [None] * len(grid_times)
    
    # –°–æ–∑–¥–∞—ë–º –∏–Ω–¥–µ–∫—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∏–∑–≤–µ—Å—Ç–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
    time_value_pairs = sorted(zip(times, values))
    
    result = []
    max_gap = timedelta(seconds=max_gap_seconds)
    
    for grid_time in grid_times:
        # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ –∏–∑–≤–µ—Å—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ <= grid_time
        last_value = None
        last_time = None
        
        for t, v in time_value_pairs:
            if t <= grid_time:
                last_value = v
                last_time = t
            else:
                break
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑—Ä—ã–≤
        if last_time is None:
            result.append(None)
        elif grid_time - last_time > max_gap:
            result.append(None)
        else:
            result.append(last_value)
    
    return result

# –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
data_by_type = defaultdict(lambda: {"values": [], "times": []})
data_by_entity = defaultdict(lambda: {"values": [], "times": [], "type": None, "location": None})

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
stats = {
    "total_rows": 0,
    "header_rows": 0,
    "empty_rows": 0,
    "unknown_type_rows": 0,
    "invalid_value_rows": 0,
    "invalid_time_rows": 0,
    "processed_rows": 0
}

dialect = sniff_dialect(CSV_FILE)

print(f"\n{'='*60}")
print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {CSV_FILE}...")
print(f"{'='*60}\n")

with open(CSV_FILE, "r", encoding="utf-8", newline="") as f:
    reader = csv.reader(f, dialect)

    for row in reader:
        stats["total_rows"] += 1
        
        if not row or len(row) < 3:
            stats["empty_rows"] += 1
            continue

        entity_id = (row[0] or "").strip()
        state = (row[1] or "").strip()
        timestamp = (row[2] or "").strip()

        # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
        low1, low2, low3 = entity_id.lower(), state.lower(), timestamp.lower()
        if (low1, low2, low3) == ("column1", "column2", "column3"):
            stats["header_rows"] += 1
            continue
        if (low1, low2, low3) == ("entity_id", "state", "last_changed"):
            stats["header_rows"] += 1
            continue

        # –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö
        data_type = detect_data_type(entity_id)
        if not data_type:
            stats["unknown_type_rows"] += 1
            continue

        try:
            v = clean_ha_number(state)
        except Exception:
            stats["invalid_value_rows"] += 1
            continue
        
        try:
            t = parse_time(timestamp)
        except Exception:
            stats["invalid_time_rows"] += 1
            continue
        
        # –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ
        stats["processed_rows"] += 1
        data_by_type[data_type]["values"].append(v)
        data_by_type[data_type]["times"].append(t)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ entity —Å –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        data_by_entity[entity_id]["values"].append(v)
        data_by_entity[entity_id]["times"].append(t)
        if data_by_entity[entity_id]["type"] is None:
            data_by_entity[entity_id]["type"] = data_type
            data_by_entity[entity_id]["location"] = detect_location(entity_id)

print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ CSV:")
print(f"   –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {stats['total_rows']}")
print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['processed_rows']}")
if stats["header_rows"] > 0:
    print(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ (–∑–∞–≥–æ–ª–æ–≤–∫–∏): {stats['header_rows']}")
if stats["empty_rows"] > 0:
    print(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ (–ø—É—Å—Ç—ã–µ): {stats['empty_rows']}")
if stats["unknown_type_rows"] > 0:
    print(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø): {stats['unknown_type_rows']}")
if stats["invalid_value_rows"] > 0:
    print(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è): {stats['invalid_value_rows']}")
if stats["invalid_time_rows"] > 0:
    print(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –≤—Ä–µ–º—è): {stats['invalid_time_rows']}")
print()

if not data_by_type:
    print("‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –¥–∞–Ω–Ω—ã–µ –Ω–∏ –æ–¥–Ω–æ–≥–æ –∏–∑ —Ç–∏–ø–æ–≤:")
    for name in DATA_PATTERNS.keys():
        print(f"  - {name}")
    print("\n–ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ CSV —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω—É–∂–Ω—ã–µ entity_id")
    exit(1)

# –°–Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–≤–æ–π–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏, —á—Ç–æ–±—ã –∑–Ω–∞—Ç—å –∫–∞–∫–∏–µ —Ç–∏–ø—ã –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å
dual_files = []
created_files = []
dual_types_found = set()  # —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö —Å–æ–∑–¥–∞–Ω –¥–≤–æ–π–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫

for chart_key, chart_config in DUAL_CHARTS.items():
    data_type = chart_config["type"]
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ entity —ç—Ç–æ–≥–æ —Ç–∏–ø–∞ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –ª–æ–∫–∞—Ü–∏–µ–π
    inside_candidates = []
    outside_candidates = []
    unknown_location = []
    
    for entity_id, entity_data in data_by_entity.items():
        if entity_data["type"] != data_type:
            continue
        
        location = entity_data["location"]
        count = len(entity_data["values"])
        
        candidate = {
            "entity_id": entity_id,
            "count": count,
            "data": entity_data
        }
        
        if location == "inside":
            inside_candidates.append(candidate)
        elif location == "outside":
            outside_candidates.append(candidate)
        else:
            unknown_location.append(candidate)
    
    # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    if not inside_candidates or not outside_candidates:
        print(f"‚ö† –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {chart_key}: –Ω–∞–π–¥–µ–Ω–æ inside={len(inside_candidates)}, outside={len(outside_candidates)}")
        if inside_candidates:
            print(f"   Inside –∫–∞–Ω–¥–∏–¥–∞—Ç—ã:")
            for c in inside_candidates:
                print(f"      - {c['entity_id']} ({c['count']} —Ç–æ—á–µ–∫)")
        if outside_candidates:
            print(f"   Outside –∫–∞–Ω–¥–∏–¥–∞—Ç—ã:")
            for c in outside_candidates:
                print(f"      - {c['entity_id']} ({c['count']} —Ç–æ—á–µ–∫)")
        if unknown_location:
            print(f"   –ù–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–∞—è –ª–æ–∫–∞—Ü–∏—è (–¥–æ–±–∞–≤—å—Ç–µ –º–∞—Ä–∫–µ—Ä—ã!):")
            for c in unknown_location:
                print(f"      - {c['entity_id']} ({c['count']} —Ç–æ—á–µ–∫)")
        continue
    
    best_inside = max(inside_candidates, key=lambda x: x["count"])
    best_outside = max(outside_candidates, key=lambda x: x["count"])
    
    candidates = {
        "Inside": best_inside,
        "Outside": best_outside
    }
    
    print(f"‚úì {chart_key}: –≤—ã–±—Ä–∞–Ω—ã —Å–µ–Ω—Å–æ—Ä—ã")
    print(f"   Inside:  {best_inside['entity_id']} ({best_inside['count']} —Ç–æ—á–µ–∫)")
    print(f"   Outside: {best_outside['entity_id']} ({best_outside['count']} —Ç–æ—á–µ–∫)")
    
    # –°—Ç—Ä–æ–∏–º –æ–±—â—É—é —Å–µ—Ç–∫—É –≤—Ä–µ–º–µ–Ω–∏
    all_times = set()
    for member_data in candidates.values():
        all_times.update(member_data["data"]["times"])
    
    grid_times = sorted(all_times)
    
    # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å–µ—Ä–∏–∏ - —Ñ–∏–ª—å—Ç—Ä—É–µ–º null –∑–Ω–∞—á–µ–Ω–∏—è
    series = []
    all_values = []
    
    for member_name in ["Inside", "Outside"]:  # –ø–æ—Ä—è–¥–æ–∫ –≤–∞–∂–µ–Ω –¥–ª—è –≤—ã–≤–æ–¥–∞
        member = candidates[member_name]
        aligned_points = build_aligned_series(
            member["data"],
            grid_times,
            chart_config["max_gap_seconds"]
        )
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º null - –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ —Ç–æ—á–∫–∏
        valid_points = [round(v, 2) for v in aligned_points if v is not None]
        
        series.append({
            "name": member_name,
            "unit": chart_config["unit"],
            "points": valid_points
        })
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ min/max
        all_values.extend(valid_points)
    
    if not all_values:
        print(f"‚ö† –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {chart_key}: –Ω–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")
        continue
    
    # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–∏–µ min/max
    min_val = round(min(all_values) - 10)
    max_val = round(max(all_values) + 10)
    
    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–ª—è –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤
    if chart_config["unit"] == "%":
        min_val = max(0, min_val)
        max_val = min(100, max_val)
    
    dual_output = {
        "min": min_val,
        "max": max_val,
        "series": series
    }
    
    out_file = chart_config["output"]
    with open(out_file, "w", encoding="utf-8") as f:
        json.dump(dual_output, f, indent=2)
    
    dual_types_found.add(data_type)  # –æ—Ç–º–µ—á–∞–µ–º, —á—Ç–æ –¥–ª—è —ç—Ç–æ–≥–æ —Ç–∏–ø–∞ —Å–æ–∑–¥–∞–Ω –¥–≤–æ–π–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫
    
    dual_files.append({
        "key": chart_key,
        "file": out_file,
        "members": {name: data["entity_id"] for name, data in candidates.items()},
        "original_counts": {name: data["count"] for name, data in candidates.items()},
        "grid_points": len(grid_times),
        "range": (min_val, max_val)
    })

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤ (—Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–∏–ø–æ–≤ –±–µ–∑ –¥–≤–æ–π–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤)

for data_type, data in data_by_type.items():
    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–∏–ø—ã, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö —É–∂–µ —Å–æ–∑–¥–∞–Ω –¥–≤–æ–π–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫
    if data_type in dual_types_found:
        continue
    
    values = data["values"]
    times = data["times"]
    
    if not values:
        continue
    
    # —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    sorted_values = [v for _, v in sorted(zip(times, values))]
    
    output_data = {
        "min": round(min(sorted_values) - 10),
        "max": round(max(sorted_values) + 10),
        "points": [round(v, 2) for v in sorted_values]
    }
    
    out_file = DATA_PATTERNS[data_type]["file"]
    
    with open(out_file, "w", encoding="utf-8") as f:
        json.dump(output_data, f, indent=2)
    
    created_files.append({
        "type": data_type,
        "file": out_file,
        "count": len(sorted_values),
        "range": (output_data["min"], output_data["max"]),
        "preview": output_data["points"][:3]
    })

# –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
print(f"\n{'='*60}")
print(f"‚úì –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤: {len(created_files)}\n")

for info in created_files:
    print(f"üìä {info['type']}")
    print(f"   –§–∞–π–ª: {info['file']}")
    print(f"   –¢–æ—á–µ–∫: {info['count']}")
    print(f"   –î–∏–∞–ø–∞–∑–æ–Ω: {info['range'][0]} ... {info['range'][1]}")
    print(f"   –ü–µ—Ä–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {info['preview']}")
    print()

if dual_files:
    print(f"‚úì –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–≤–æ–π–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤: {len(dual_files)}\n")
    
    for info in dual_files:
        print(f"üìà {info['key']} (–¥–≤–æ–π–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫)")
        print(f"   –§–∞–π–ª: {info['file']}")
        for member_name, entity_id in info['members'].items():
            orig_count = info['original_counts'][member_name]
            print(f"   {member_name}: {entity_id} ({orig_count} —Ç–æ—á–µ–∫)")
        print(f"   –û–±—â–∞—è —Å–µ—Ç–∫–∞: {info['grid_points']} —Ç–æ—á–µ–∫")
        print(f"   –î–∏–∞–ø–∞–∑–æ–Ω: {info['range'][0]} ... {info['range'][1]}")
        print()

# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Ç–∏–ø–∞—Ö
found_types = set(data_by_type.keys())
all_types = set(DATA_PATTERNS.keys())
missing_types = all_types - found_types

if missing_types:
    print(f"‚ÑπÔ∏è  –ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è: {', '.join(missing_types)}")

print(f"{'='*60}\n")