import csv
import json
import re
from datetime import datetime
from collections import defaultdict

CSV_FILE = "history.csv"

# –ü—Ä–∞–≤–∏–ª–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö –ø–æ entity_id
DATA_PATTERNS = {
    "batteryLevel": {
        "keywords": ["battery_level", "battery"],
        "file": "ha-battery.json"
    },
    "temperature": {
        "keywords": ["temperature", "_temp"],
        "file": "ha-temperature.json"
    },
    "chargeCurrent": {
        "keywords": ["current", "charge_current"],
        "file": "ha-charge.json"
    },
    "humidity": {
        "keywords": ["humidity", "relative_humidity"],
        "file": "ha-humidity.json"
    }
}

def detect_data_type(entity_id: str) -> str | None:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –ø–æ entity_id"""
    entity_lower = entity_id.lower()
    
    for data_type, config in DATA_PATTERNS.items():
        for keyword in config["keywords"]:
            if keyword in entity_lower:
                return data_type
    return None

def parse_time(s: str) -> datetime:
    s = (s or "").strip()
    s = s.replace("Z", "")
    return datetime.fromisoformat(s)

def clean_ha_number(s: str) -> float:
    """
    –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫–∏ –≤–∏–¥–∞:
      -12.299.999.949.722.200  -> -12.2999999497222
       20.251.068.571.847.200  ->  20.2510685718472
    –∏ –æ–±—ã—á–Ω—ã–µ —á–∏—Å–ª–∞ —Ç–æ–∂–µ –ø–æ–Ω–∏–º–∞–µ—Ç.
    """
    s = (s or "").strip()
    if not s:
        raise ValueError("empty number")

    # –µ—Å–ª–∏ —É–∂–µ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π float ‚Äî –ø—Ä–æ—Å—Ç–æ –ø–∞—Ä—Å–∏–º
    try:
        return float(s)
    except Exception:
        pass

    # –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã, –∑–Ω–∞–∫ –∏ —Ç–æ—á–∫–∏
    s = re.sub(r"[^0-9\.\-+]", "", s)

    # –µ—Å–ª–∏ —Ç–æ—á–µ–∫ 0 –∏–ª–∏ 1 ‚Äî —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Å–ª—É—á–∞–π
    if s.count(".") <= 1:
        return float(s)

    # –µ—Å–ª–∏ —Ç–æ—á–µ–∫ –º–Ω–æ–≥–æ: —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ –ü–ï–†–í–ê–Ø —Ç–æ—á–∫–∞ ‚Äî –¥–µ—Å—è—Ç–∏—á–Ω–∞—è,
    # –æ—Å—Ç–∞–ª—å–Ω—ã–µ ‚Äî –º—É—Å–æ—Ä–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ ‚Üí —É–±–∏—Ä–∞–µ–º
    sign = ""
    if s[0] in "+-":
        sign, s = s[0], s[1:]

    first_dot = s.find(".")
    if first_dot == -1:
        return float(sign + s)

    int_part = s[:first_dot]
    frac_part = s[first_dot+1:].replace(".", "")
    # –∑–∞—â–∏—Ç–∞: –µ—Å–ª–∏ –≤–¥—Ä—É–≥ int_part –ø—É—Å—Ç–æ–π
    if int_part == "":
        int_part = "0"

    return float(f"{sign}{int_part}.{frac_part}")

def sniff_dialect(path: str):
    with open(path, "r", encoding="utf-8", newline="") as f:
        sample = f.read(4096)
    return csv.Sniffer().sniff(sample, delimiters=",;\t")

# –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ —Ç–∏–ø–∞–º
data_by_type = defaultdict(lambda: {"values": [], "times": []})

dialect = sniff_dialect(CSV_FILE)

print(f"\n{'='*60}")
print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {CSV_FILE}...")
print(f"{'='*60}\n")

with open(CSV_FILE, "r", encoding="utf-8", newline="") as f:
    reader = csv.reader(f, dialect)

    for row in reader:
        if not row or len(row) < 3:
            continue

        entity_id = (row[0] or "").strip()
        state = (row[1] or "").strip()
        timestamp = (row[2] or "").strip()

        # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
        low1, low2, low3 = entity_id.lower(), state.lower(), timestamp.lower()
        if (low1, low2, low3) == ("column1", "column2", "column3"):
            continue
        if (low1, low2, low3) == ("entity_id", "state", "last_changed"):
            continue

        # –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö
        data_type = detect_data_type(entity_id)
        if not data_type:
            continue

        try:
            v = clean_ha_number(state)
            t = parse_time(timestamp)
            data_by_type[data_type]["values"].append(v)
            data_by_type[data_type]["times"].append(t)
        except Exception:
            continue

if not data_by_type:
    print("‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –¥–∞–Ω–Ω—ã–µ –Ω–∏ –æ–¥–Ω–æ–≥–æ –∏–∑ —Ç–∏–ø–æ–≤:")
    for name in DATA_PATTERNS.keys():
        print(f"  - {name}")
    print("\n–ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ CSV —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω—É–∂–Ω—ã–µ entity_id")
    exit(1)

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö
created_files = []

for data_type, data in data_by_type.items():
    values = data["values"]
    times = data["times"]
    
    if not values:
        continue
    
    # —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    sorted_values = [v for _, v in sorted(zip(times, values))]
    
    output_data = {
        "min": round(min(sorted_values) - 10),
        "max": round(max(sorted_values) + 10),
        "points": [round(v, 2) for v in sorted_values]
    }
    
    out_file = DATA_PATTERNS[data_type]["file"]
    
    with open(out_file, "w", encoding="utf-8") as f:
        json.dump(output_data, f, indent=2)
    
    created_files.append({
        "type": data_type,
        "file": out_file,
        "count": len(sorted_values),
        "range": (output_data["min"], output_data["max"]),
        "preview": output_data["points"][:3]
    })

# –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
print(f"‚úì –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö: {len(created_files)}\n")

for info in created_files:
    print(f"üìä {info['type']}")
    print(f"   –§–∞–π–ª: {info['file']}")
    print(f"   –¢–æ—á–µ–∫: {info['count']}")
    print(f"   –î–∏–∞–ø–∞–∑–æ–Ω: {info['range'][0]} ... {info['range'][1]}")
    print(f"   –ü–µ—Ä–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {info['preview']}")
    print()

# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Ç–∏–ø–∞—Ö
found_types = set(data_by_type.keys())
all_types = set(DATA_PATTERNS.keys())
missing_types = all_types - found_types

if missing_types:
    print(f"‚ÑπÔ∏è  –ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è: {', '.join(missing_types)}")

print(f"{'='*60}\n")